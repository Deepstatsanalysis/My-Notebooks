{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: text_preprocessor in c:\\users\\eazetng\\anaconda3\\lib\\site-packages (0.0.2)\n",
      "Requirement already satisfied: jieba in c:\\users\\eazetng\\anaconda3\\lib\\site-packages (from text_preprocessor) (0.40)\n",
      "Requirement already satisfied: keras in c:\\users\\eazetng\\anaconda3\\lib\\site-packages (from text_preprocessor) (2.2.4)\n",
      "Requirement already satisfied: numpy in c:\\users\\eazetng\\anaconda3\\lib\\site-packages (from text_preprocessor) (1.18.0)\n",
      "Requirement already satisfied: scipy>=0.14 in c:\\users\\eazetng\\anaconda3\\lib\\site-packages (from keras->text_preprocessor) (1.1.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in c:\\users\\eazetng\\anaconda3\\lib\\site-packages (from keras->text_preprocessor) (1.0.9)\n",
      "Requirement already satisfied: six>=1.9.0 in c:\\users\\eazetng\\anaconda3\\lib\\site-packages (from keras->text_preprocessor) (1.11.0)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in c:\\users\\eazetng\\anaconda3\\lib\\site-packages (from keras->text_preprocessor) (1.0.7)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\eazetng\\anaconda3\\lib\\site-packages (from keras->text_preprocessor) (3.12)\n",
      "Requirement already satisfied: h5py in c:\\users\\eazetng\\anaconda3\\lib\\site-packages (from keras->text_preprocessor) (2.7.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using pip version 19.0.3, however version 19.3.1 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'TextPreprocessor'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-d39879a56a59>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mload_model\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtext_preprocessor\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mTextPreprocessor\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclassification_report\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'TextPreprocessor'"
     ]
    }
   ],
   "source": [
    "!pip install text_preprocessor \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Input, Dense, Flatten, Conv1D, MaxPooling1D, Dropout, LSTM\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras import activations, initializers, regularizers, constraints\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.utils import np_utils\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import load_model\n",
    "\n",
    "from text_preprocessor import TextPreprocessor\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from keras import optimizers\n",
    "\n",
    "VECTORIZER_FILE = 'models/vect_lstm_pretrained_lr2.pkl'\n",
    "MODEL_FILE = 'models/sentiment_lstm_pretrained_lr2.hdf5'\n",
    "CHECKPOINT_FILE = 'models/sentiment_lstm_weights_pretrained_lr2.hdf5'\n",
    "GLOVE = 'data/glove.twitter.27B.100d.txt'\n",
    "\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE= 128\n",
    "EMBED_DIMS = 100\n",
    "\n",
    "MAX_LEN = 60\n",
    "\n",
    "class SentimentAnalysisLstm:\n",
    "  def __init__(self, train_filename='data/full_preprocessed_sergio.csv', test_filename='data/minnesota_test.csv', is_brand=True):\n",
    "    columns=['id', 'Query Name', 'text', 'label', 'clean_text']\n",
    "    self.df = pd.read_csv(train_filename, header=None, names=columns, encoding = \"ISO-8859-1\")\n",
    "\n",
    "    self.df = self.df[pd.notnull(self.df['clean_text'])]\n",
    "\n",
    "    self.df['max_len'] = self.df['clean_text'].apply(lambda x: len(x))\n",
    "    self.max_len = 290 # self.df['max_len'].max() + 1 ## requires more memory\n",
    "    print('max sentence length', self.df['max_len'].max())\n",
    "\n",
    "    self.dftest = self.read_brand_test_data(test_filename, is_brand)\n",
    "    self.vect = None\n",
    "\n",
    "  def train(self):\n",
    "    filepath=\"models/{}\".format(CHECKPOINT_FILE)\n",
    "\n",
    "    X = self.word_embeddings(self.df['clean_text'].values, self.max_len)\n",
    "    y = np_utils.to_categorical(self.df['label'].values)\n",
    "\n",
    "    X_train, X_val, Y_train, Y_val = train_test_split(X,y, test_size = 0.3, random_state = 42)\n",
    "\n",
    "    self.embed_dict = self.create_word_embeddings_dict()\n",
    "    self.embed_matrix = self.create_word_embeddings_matrix(self.embed_dict)\n",
    "\n",
    "    model = self.build_model(self.max_len)\n",
    "\n",
    "    checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "\n",
    "    history = model.fit(X_train, Y_train, batch_size=BATCH_SIZE, epochs=EPOCHS, validation_data=(X_val, Y_val), callbacks = [checkpoint])\n",
    "\n",
    "    model.save(\"saved_models/{}\".format(MODEL_FILE))\n",
    "\n",
    "    score,acc = model.evaluate(X_val, Y_val, verbose = 2, batch_size = BATCH_SIZE)\n",
    "    print(\"score: %.2f\" % (score))\n",
    "    print(\"acc: %.2f\" % (acc))\n",
    "\n",
    "  def build_model(self, max_len):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(self.vocab_size, EMBED_DIMS, input_length=max_len, weights=[self.embed_matrix], trainable=False))\n",
    "\n",
    "    #model.add(Dropout(0.5))\n",
    "    model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "    model.add(Dense(3,activation='softmax'))\n",
    "\n",
    "    # adam default parameters:  lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0.\n",
    "    adam = optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "  def create_word_embeddings_matrix(self, embed_dict):\n",
    "    emb_matrix = np.zeros((self.vocab_size, EMBED_DIMS))\n",
    "    for w, i in self.tokenizer.word_index.items():\n",
    "        if i < self.vocab_size:\n",
    "            vect = embed_dict.get(w)\n",
    "            if vect is not None:\n",
    "              emb_matrix[i] = vect\n",
    "        else:\n",
    "            break\n",
    "    return emb_matrix\n",
    "\n",
    "  def create_word_embeddings_dict(self):\n",
    "    filename = \"data/{}\".format(GLOVE)\n",
    "    emb_dict = {}\n",
    "    glove = open(filename, 'r', encoding = \"utf-8\")\n",
    "    for line in glove:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vector = np.asarray(values[1:], dtype='float32')\n",
    "        emb_dict[word] = vector\n",
    "    glove.close()\n",
    "    return emb_dict\n",
    "\n",
    "  def word_embeddings(self, texts, max_len):\n",
    "    # self.tokenizer = Tokenizer(num_words=max_fatures)\n",
    "    self.tokenizer = Tokenizer()\n",
    "    self.tokenizer.fit_on_texts(texts)\n",
    "\n",
    "    sequences = self.tokenizer.texts_to_sequences(texts)\n",
    "    self.vocab_size = len(self.tokenizer.word_index) + 1\n",
    "    print('Found %d unique words.' % len(self.tokenizer.word_index))\n",
    "\n",
    "    x_train = pad_sequences(sequences, maxlen=max_len)\n",
    "\n",
    "    with open(\"models/{}\".format(VECTORIZER_FILE), 'wb') as handle:\n",
    "      pickle.dump(self.tokenizer, handle, protocol = pickle.HIGHEST_PROTOCOL)\n",
    "      print ('tokenizer saved')\n",
    "\n",
    "    return x_train\n",
    "\n",
    "  def read_brand_test_data(self, test_filename, is_brand=True):\n",
    "\n",
    "    if is_brand:\n",
    "      df = pd.read_csv(test_filename)\n",
    "      df2 = pd.read_csv('data/minnesota_test2.csv')\n",
    "      df = df.append(df2, ignore_index=True, sort=True)\n",
    "      df = df[~df['Sentiment'].apply(self.is_not_ascii)]\n",
    "      df = df.rename(columns={'Snippet': 'text', 'Sentiment': 'label'})\n",
    "\n",
    "      #df = df[df.label != 'neutral']\n",
    "      #df['label'] = df['label'].apply(lambda x: 0 if x == 'negative' else 4)\n",
    "      di = { 'positive': 2, 'neutral': 1, 'negative': 0 }\n",
    "      df[\"label\"].replace(di, inplace=True)\n",
    "    else:\n",
    "      columns=['label', 'id', 'created_at', 'query', 'user', 'text']\n",
    "      df = pd.read_csv(test_filename, header=None, names=columns)\n",
    "      df = df[(df.label == 4) | (df.label == 0)]\n",
    "\n",
    "    return df\n",
    "\n",
    "  def load_pretrained_model(self):\n",
    "    self.model = load_model(\"models/{}\".format(MODEL_FILE))\n",
    "    with open(\"models/{}\".format(VECTORIZER_FILE), 'rb') as f2:\n",
    "      self.vect = pickle.load(f2)\n",
    "    return self.model\n",
    "\n",
    "  def predict(self, model):\n",
    "    tqdm.pandas()\n",
    "    print('preprocessing test data...')\n",
    "    tp = TextPreprocessor()\n",
    "    self.dftest['clean_text'] = self.dftest['text'].progress_apply(tp.pre_process_text_no_stemming)\n",
    "\n",
    "    print('word embeddings test data...')\n",
    "    sequences = self.vect.texts_to_sequences(self.dftest['clean_text'].values)\n",
    "\n",
    "    X_test = pad_sequences(sequences, maxlen=self.max_len)\n",
    "    y_test = self.dftest['label'].values\n",
    "    print('predict...')\n",
    "\n",
    "    preds = model.predict(X_test)\n",
    "    y_preds = [self.prob_to_sentiment_label(pred) for pred in preds]\n",
    "\n",
    "    prob_map = ['negative', 'neutral', 'positive']\n",
    "\n",
    "    probs = []\n",
    "    for pred in preds:\n",
    "      di = {}\n",
    "      for i, prob in enumerate(pred):\n",
    "        di[prob_map[i]] = prob\n",
    "      probs.append(di)\n",
    "\n",
    "    ##probs = [\"{}:{}\".format(prob_map[i[0]], prob) for i, prob in enumerate(preds)]\n",
    "\n",
    "    self.dftest['pred'] = y_preds\n",
    "    self.dftest['prob'] = probs\n",
    "\n",
    "    submission = self.dftest[['text', 'label', 'pred', 'prob']]\n",
    "\n",
    "    submission.to_csv('data/predictions_3_categories.csv')\n",
    "\n",
    "    # print(classification_report(y_test, y_preds))\n",
    "\n",
    "    score,acc = model.evaluate(X_test, np_utils.to_categorical(self.dftest['label'].values), verbose = 2, batch_size = 128)\n",
    "    print(\"score: %.2f\" % (score))\n",
    "    print(\"acc: %.2f\" % (acc))\n",
    "\n",
    "    return y_preds\n",
    "\n",
    "  def predict_single_text(self, model, text):\n",
    "    sequences = self.vect.texts_to_sequences([text])\n",
    "    X_test = pad_sequences(sequences, maxlen=self.max_len)\n",
    "    print('predict...')\n",
    "    pred = model.predict(X_test)[0]\n",
    "    prob_map = ['NEGATIVE', 'NEUTRAL', 'POSITIVE']\n",
    "    print('****************')\n",
    "    print(prob_map[np.argmax(pred)])\n",
    "    print('****************')\n",
    "\n",
    "  def prob_to_sentiment_label(self, pred):\n",
    "    #THRESHOLD = .4\n",
    "    #return 0 if pred[0] > THRESHOLD else 1\n",
    "\n",
    "    return np.argmax(pred)\n",
    "\n",
    "  def decode_sentiment(self, pred):\n",
    "    return 'POSITIVE' if self.prob_to_sentiment_label(pred) == 1 else 'NEGATIVE'\n",
    "\n",
    "  def is_not_ascii(self, string):\n",
    "    return string is not None and any([ord(s) >= 128 for s in string])\n",
    "\n",
    "  def preds(self):\n",
    "    df = pd.read_csv('data/predictions_3_categories.csv')\n",
    "\n",
    "    correct = 0\n",
    "    incorrect = 0\n",
    "\n",
    "    for i, row in df.iterrows():\n",
    "      if (row['label'] == row['pred']):\n",
    "        correct += 1\n",
    "      else:\n",
    "        incorrect += 1\n",
    "    print('correct', correct)\n",
    "    print('incorrect', incorrect)\n",
    "\n",
    "    print(correct / df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "'''\n",
    "  --------------------------------------------------\n",
    "  MAIN\n",
    "  --------------------------------------------------\n",
    "'''\n",
    "\n",
    "# if len(sys.argv) == 1:\n",
    "#   print(\"task name is required. USAGE: python3 <filename> <task>\")\n",
    "# elif sys.argv[1] == 'train':\n",
    "#   analyzer = SentimentAnalysisLstm()\n",
    "#   analyzer.train()\n",
    "# elif sys.argv[1] == 'test':\n",
    "#   analyzer = SentimentAnalysisLstm(train_filename='data/full_no_stem_preprocessed.csv', test_filename='data/minnesota_test.csv', is_brand=True)\n",
    "#   model = analyzer.load_pretrained_model()\n",
    "#   preds = analyzer.predict(model)\n",
    "# elif sys.argv[1] == 'debug':\n",
    "analyzer = SentimentAnalysisLstm()\n",
    "model = analyzer.load_pretrained_model()\n",
    "analyzer.predict_single_text(model, \"The sun is shining, everything is beautiful and I'm happy\")\n",
    "# elif sys.argv[1] == 'preds':\n",
    "#   analyzer = SentimentAnalysisLstm()\n",
    "#   analyzer.preds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
